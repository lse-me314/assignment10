---
title: "Assignment 9 - Working With Textual Data (Solutions)"
author: "Jack Blumenau"
output: html_document
---

This exercise is designed to get you working with [quanteda](https://quanteda.io) and some other associated packages.  The focus will be on exploring the package, getting some texts into the **corpus** object format, learning how to convert texts into document-feature matrices, and performing descriptive analyses on this data.  

#### 1. Getting Started.

1. You will first need to install and load the following packages: 

```{r, eval=FALSE}
install.packages("quanteda")
install.packages("readtext")
install.packages("quanteda.textplots")
install.packages("quanteda.textstats")
```

```{r, echo = T, message=FALSE}
library(quanteda)
library(quanteda.textplots)
library(quanteda.textstats)
library(readtext)
```

2. You will also need to install the package `quanteda.corpora` from github using the `install_github` function from the `remotes` package: 

```{r, eval=FALSE}
remotes::install_github("quanteda/quanteda.corpora")
library(quanteda.corpora)
```
    
    
3. Exploring **quanteda** functions. Look at the Quick Start vignette, and browse the manual for quanteda.  You can use `example()` function for any function in the package, to run the examples and see how the function works.  Of course you should also browse the documentation, especially `?corpus` to see the structure and operations of how to construct a corpus.  The website http://quanteda.io has extensive     documentation.

```{r, eval = F}
?corpus
example(dfm)
example(corpus)
```


#### 2. Making a corpus and corpus structure

There are several ways of converting a set of texts into the `corpus` object structure that is required for subsequent analyses in quanteda. We discuss several ways here.

1.  *From a vector of texts already in memory*. The simplest way to create a corpus is to use a vector of texts already present in R's global environment. Some text and corpus objects are built into the package, for example `data_char_ukimmig2010` is the UTF-8 encoded set of 9 UK party manifesto sections from 2010, that deal with immigration policy. addresses.  Try using `corpus()` on this set of texts to create a corpus.  
      
Once you have constructed this corpus, use the `summary()` method to see a brief description of the corpus.  The names of the corpus `data_char_ukimmig2010` should have become the document names.

```{r}
immig_corpus <- corpus(data_char_ukimmig2010)
summary(immig_corpus)
```

2. *From a directory of text files*. The `readtext()` function from the **readtext** package can read (almost) any set of files into an object that you can then call the `corpus()` function on, to create a corpus.  (See `?readtext` for an example.)
      
Here you are encouraged to select any directory of plain text files of your own.  How did it work?  Try using `docvars()` to assign a set of document-level variables.
        
If you do not have a set of text files to work with, then you can use the UK 2010 manifesto texts on immigration, in the Day 8 folder, like this:
      
```{r, echo=FALSE}
library("quanteda", quietly = TRUE, warn.conflicts = FALSE)
```

```{r, eval=FALSE}
manfiles <- readtext("https://github.com/lse-me314/assignment09/blob/master/UKimmigTexts.zip?raw=true")
mycorpus <- corpus(manfiles)
```
   
3. *From `.csv` or `.json` files* --- see the documentation for the package `readtext` (`help(package = "readtext")`). Here you can try one of your own examples, or just file this in your mental catalog for future reference.

For the rest of this assignment, we will describe the functions that you can use in quanteda using the `data_corpus_inaugural` data which is pre-loaded by the `quanteda` package. This data includes the texts of 59 US presidential inaugural address texts, and metadata (for the corpus), from 1789 to present. You can follow along with these steps, or you can use any data of your own that you have managed to load into R using one of the above methods.

#### 3. Tokenizing texts

In order to count word frequencies, we first need to split the text into words (or longer phrases) through a process known as *tokenization*.  Look at the documentation for **quanteda**'s `tokens()` function.  

1. Use the `tokens` command on `data_corpus_inaugural` object, and examine the results. 

```{r}

inaugural_tokens <- tokens(data_corpus_inaugural)

```

2. Experiment with some of the arguments of the `tokens()` function, such as `remove_punct` and `remove_numbers`.

```{r}

inaugural_tokens <- tokens(data_corpus_inaugural, remove_punct = TRUE, remove_numbers = TRUE)

```

3. Try tokenizing the *sentences* from `data_corpus_inaugural` into sentences, using `tokens(x, what = "sentence")`. 

```{r, eval = F}

inaugural_sentences <- tokens(data_corpus_inaugural, what = "sentence")
inaugural_sentences[1:2]

```

#### 4. Explore some phrases in the text.  

1. **quanteda** provides a keyword-in-context function that is easily usable and configurable to explore texts in a descriptive way. Use the `kwic()` function (for "keywords-in-context") to explore how a specific word or phrase is used in this corpus (use the word-based tokenization that you implemented above). You can look at the help file (`?kwic`) to see the arguments that the function takes.
      
```{r}
kwic(inaugural_tokens, "terror", 3)
```

Try substituting your own search terms, or working with your own corpus.

```{r, eval = T}

head(kwic(inaugural_tokens, "america", 3))

head(kwic(inaugural_tokens, "democracy", 3))
```

2. By default, kwic gives exact matches for a given pattern. What if we wanted to see words like "terrorism" and "terrorist" rather than exactly "terror"? We can use the wildcard character `*` to exand our search by appending it to the end of the pattern we are using to search. For example, we could use `"terror*"`. Try this now in the `kwic()` function.

```{r}
kwic(inaugural_tokens, "terror*", 3)
```

#### 5. Creating a `dfm()`

Document-feature matrices are the standard way of representing text as quantitative data. Fortunately, it is very simple to convert the tokens objects in quanteda into dfms.

1. Create a document-feature matrix, using `dfm` applied to the `immig_tokens` object you created above.  First, read the documentation using `?dfm` to see the available options. Once you have created the dfm, use the `topfeatures()` function to inspect the top 20 most frequently occuring features in the dfm. What kinds of words do you see?
   
```{r}
mydfm <- dfm(inaugural_tokens)
mydfm
topfeatures(mydfm, 20)
```

**Mostly stop words!**
   
2. Experiment with different `dfm_*` functions, such as `dfm_wordstem()`, `dfm_remove()` and `dfm_trim()`. These functions allow you to reduce the size of the dfm following its construction. How does the number of features in your dfm change as you apply these functions to the dfm object you created in the question above?
    
```{r}
dim(mydfm)
dim(dfm_wordstem(mydfm))
dim(dfm_remove(mydfm, pattern = stopwords("english")))
dim(dfm_trim(mydfm, min_termfreq = 5, min_docfreq = 0.01, termfreq_type = "count", docfreq_type = "prop"))
```

3. Use the `dfm_remove()` function to remove English-language stopwords from this data. You can get a list of English stopwords by using `stopwords("english")`.
   
```{r}
mydfm_nostops <- dfm_remove(mydfm, pattern = stopwords("en"))
mydfm_nostops
```
   
4.  You can easily use quanteda to subset a corpus. There is a `corpus_subset()` method defined for a corpus, which works just like R's normal `subset()` command.  For instance if you want a wordcloud of just Obama's two inaugural addresses, you would need to subset the corpus first:
   
```{r}
obama_corpus <- corpus_subset(data_corpus_inaugural, President == "Obama")
obama_tokens <- tokens(obama_corpus)
obama_dfm <- dfm(obama_tokens)
textplot_wordcloud(obama_dfm)
```

Try producing that plot without the stopwords and without punctuation. To remove stopwords, use `dfm_remove()`. To remove punctuation, pass `remove_punct = TRUE` to the `tokens()` function.

```{r}
obama_tokens <- tokens(obama_corpus, remove_punct = TRUE)
obama_dfm <- dfm(obama_tokens)
obama_dfm <- dfm_remove(obama_dfm, pattern = stopwords("en"))
textplot_wordcloud(obama_dfm)
```

#### 6. Descriptive statistics

1.  We can plot the type-token ratio of the inaugural speeches over time. To do this, begin by summarising the speeches by each president by applying the `summary()` function to the `data_corpus_inaugural` object and examining the results.

```{r}
token_info <- summary(data_corpus_inaugural)
```

2.  Get the type-token ratio for each text, and plot the resulting vector of TTRs as a function of the `Year`  Hint: See `?textstat_lexdiv`.        

```{r}

inaugural_dfm <- dfm(inaugural_tokens, remove_punct = TRUE)
ttr_by_speech <- textstat_lexdiv(inaugural_dfm, "TTR")

plot(inaugural_dfm$Year, ttr_by_speech$TTR, main = "TTR by year", xlab = "Year", 
     ylab = "TTR", pch = 19, bty = "n", type = "b")

```


3. Use the `corpus_subset()` function to select the speeches given by presidents between 1900 and 1950. Then, using this subset, measure the term similarities (`textstat_simil`) for the following words: *economy*, *health*, *women*. Which other terms are most associated with each of these three terms?
        
```{r}
data_corpus_inaugural_subset <- corpus_subset(data_corpus_inaugural, Year %in% c(1900:1950))

inaugural_tokens_subset <- tokens(data_corpus_inaugural_subset)

inaugural_dfm_subset <- dfm(inaugural_tokens_subset)
        
word_similarities <- textstat_simil(inaugural_dfm, inaugural_dfm[,c("economy", "health", "women")], 
                                    margin = "features")

head(word_similarities[order(word_similarities[, 1], decreasing = TRUE), ])
head(word_similarities[order(word_similarities[, 2], decreasing = TRUE), ])
head(word_similarities[order(word_similarities[, 3], decreasing = TRUE), ])
```


#### 7. Working with dictionaries

1. Dictionaries are named lists, consisting of a "key" and a set of entries defining the equivalence class for the given key.  To create a simple dictionary of parts of speech, for instance we could define a dictionary consisting of articles and conjunctions, using:

```{r}

pos_dict <- dictionary(list(articles = c("the", "a", "and"),
                           conjunctions = c("and", "but", "or", "nor", "for", "yet", "so")))

```

To let this define a set of features, we can use this dictionary on the dfm object we created above. To do so, apply the `dfm_lookup()` function to the relevant dfm object, with the `dictionary` argument equal to the `pos_dict` created above:

```{r}

pos_dfm <- dfm_lookup(inaugural_dfm, dictionary = pos_dict)
pos_dfm[1:10,]

```

2. Plot the counts of articles and conjunctions (actually, here just the coordinating conjunctions) across the speeches. (**Hint:** you can use `docvars(data_corpus_inaugural, "Year"))` for the *x*-axis.) Is the distribution of normalized articles and conjunctions relatively constant across years, as you would expect?

```{r}
par(mfrow = c(1,2))
plot(data_corpus_inaugural$Year, as.numeric(pos_dfm[, 1]))
plot(data_corpus_inaugural$Year, as.numeric(pos_dfm[, 2]))
```

3. The previous analysis uses the count of articles and conjunctions, which depends on the length of the speech as longer speeches will, on average, use more articles and conjunctions. To remove this dependency, we can weight the document-feature matrix by document length and re-compute. For this, we first have to compute the full dfm (using `dfm()`), then weight it by document frequency (using `dfm_weight()` with the `scheme` argument equal to `"prop"`), and finally apply the dictionary (using `dfm_lookup()`). Apply these steps and then create a plot showing the weighted counts of articles and conjunctions over time.

```{r}

inaugural_dfm <- dfm(inaugural_tokens)
inaugural_wgt <- dfm_weight(inaugural_dfm, scheme = "prop")
pos_wgt <- dfm_lookup(inaugural_wgt, dictionary = pos_dict)

pos_wgt[1:10, ]

# For easier processing, you can turn it into a data.frame and add the Year
pos_wgt_df <- convert(pos_wgt, to = "data.frame")
pos_wgt_df <- cbind(pos_wgt_df, year = data_corpus_inaugural$Year)

par(mfrow = c(1,1))
plot(pos_wgt_df$year, pos_wgt_df$articles, type = "l", col = "orange",
     ylim = range(pos_wgt), xlab = "Year", ylab = "Weighted count")
lines(pos_wgt_df$year, pos_wgt_df$conjunctions, type = "l", col = "blue")
legend("topright", legend = c("articles", "conjunctions"), col = c("orange","blue"), lty = 1)

```

4. Create a new dictionary capturing a concept of your own choosing (perhaps something like "democracy" or "optimism"). Apply this dictionary to the inaugural speeches data and plot the prevalence of that concept in speeches made by US Presidents over time.

```{r}
optimist_dict <- dictionary(list(optimist = c("best", "better", "great", "awesome", "good", "fantastic", "terrific", "tremendous", "amazing", "superior", "astounding", "astonishing", "extraordinary", "incredible", "magnificent")))

inaugural_dfm <- dfm(inaugural_tokens)
inaugural_wgt <- dfm_weight(inaugural_dfm, scheme = "prop")
optimist_wgt <- dfm_lookup(inaugural_wgt, dictionary = optimist_dict)

# For easier processing, you can turn it into a data.frame and add the Year
optimist_wgt_df <- convert(optimist_wgt, to = "data.frame")
optimist_wgt_df <- cbind(optimist_wgt_df, year = data_corpus_inaugural$Year)

par(mfrow = c(1,1))
plot(optimist_wgt_df$year, optimist_wgt_df$optimist, type = "l", col = "orange", ylim = range(optimist_wgt), xlab = "Year", ylab = "Weighted count")

```
