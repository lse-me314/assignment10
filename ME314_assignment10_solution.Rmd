---
title: "Assignment 10 - Similarity Metrics and Supervised Learning for Text (Solutions)"
author: "Jack Blumenau"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "##"
)
```

In this assignment, you will use R to calculate similarity metrics for text data, and apply document classification methods using **quanteda**.

You will need the following packages:

```{r, echo = TRUE, message=FALSE}
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textplots)
library(quanteda.textstats)
library(tidyverse)
library(caret)
```

## Exercise 10.1 - Similarity Metrics for Analyzing the Preambles of Constitutions

Did the United States constitution influence the constitutions of other countries? There is a growing scholarly train of thought that suggests the influence of the US Constitution has decreased over time, as it is increasingly divergent from an increasing global consensus of the importance of human rights to constitutional settlements. However, there is a lack of empirical and systematic knowledge about the extent to which the U.S. Constitution impacts the revision and adoption of formal constitutions across the world.[^seminar3-1]

[^seminar3-1]: This problem set draws from material in [Quantitative Social Science: An Introduction](https://press.princeton.edu/books/hardcover/9780691167039/quantitative-social-science) by Kosuke Imai.

[David S. Law and Mila Versteeg (2012)](https://www.nyulawreview.org/wp-content/uploads/2018/08/NYULawReview-87-3-Law-Versteeg_0.pdf) investigate the influence of the US constitution empirically and show that other countries have, in recent decades, become increasingly unlikely to model the rights-related provisions of their own constitutions upon those found in the US Constitution. In this problem set, we will use some of the methods that we covered this week to replicate some parts of their analysis.

We will use the `constitutions.csv` file for this question. 

Once you have downloaded this files and stored it somewhere sensible, you can load it into R using the following command:

```{r, echo = TRUE, eval = TRUE, message=FALSE, warning=FALSE}

constitutions <- read_csv("data/constitutions.csv")

```

This file contains the preambles of `r nrow(constitutions)` (English-translated) constitutions. The data contains the following variables:

| Variable    | Description                                |
|:------------|:-------------------------------------------|
| `country`   | Name of the country                        |
| `continent` | Continent of the country                   |
| `year`      | Year in which the constitution was written |
| `preamble`  | Text of the preamble of the constitution   |

: Variables in the `constituiton` data.

You can take a quick look at the variables in the data by using the `glimpse()` function from the `tidyverse` package:

```{r, echo = TRUE, eval = TRUE}

glimpse(constitutions)

```



### Tf-idf

1.  Explore the `constitutions` object to get a sense of the data that we are working with. What is the average length of the texts stored in the `preambles` variable?[^seminar3-2] Which country has the longest preamble text?[^seminar3-3] Which has the shortest?[^seminar3-4] Has the average length of these preambles changed over time?[^seminar3-5]

[^seminar3-2]: The `ntokens()` function will be helpful here.

[^seminar3-3]: The `which.max()` function will be helpful here.

[^seminar3-4]: The `which.min()` function will be helpful here.

[^seminar3-5]: You will need to compare the length of the preambles variable to the `year` variable in some way (a good-looking plot would be nice!)

<details>

<summary>Reveal code</summary>

```{r}

# Find the length of the preambles
constitutions$preamble_length <- ntoken(constitutions$preamble)
mean(constitutions$preamble_length)

# Find the longest preamble
constitutions$country[which.max(constitutions$preamble_length)]

# Find the shortest preamble
constitutions$country[which.min(constitutions$preamble_length)]

constitutions %>%
  ggplot(aes(x = year,
             y = preamble_length)) +
  geom_point() + 
  xlab("Year") + 
  ylab("Preamble Length") + 
  theme_bw() 

```

> It is a little hard to tell whether there has been a systematic increase in preamble length across this time span because there are so few constitutions written in the earlier periods of the data. We can get a clearer view of the modern evolution of preamble length by focusing only on data since the 1950s:

```{r}
constitutions %>%
  filter(year >= 1950) %>% # Filter the observations to only those for which year >= 1950
  ggplot(aes(x = year,
             y = preamble_length)) +
  geom_point() + 
  xlab("Year") + 
  ylab("Preamble Length") + 
  theme_bw() +
  geom_smooth() # Fit a smooth loess curve to the data

```

> There is some evidence of an increase, though it is not a very clear trend.

</details>

2.  Convert the `constitutions` data.frame into a `corpus()` object and then into a `dfm()` object (remember that you will need to use the `tokens()`) function as well. Make some sensible feature selection decisions.

<details>

<summary>Reveal code</summary>

```{r}

constitutions_dfm <- constitutions %>% 
                        corpus(text_field = "preamble") %>%
                        tokens(remove_punct = T) %>%
                        dfm() %>%
                        dfm_remove(stopwords("en"))

```

</details>

3.  Use the `topfeatures()` function to find the most prevalent 10 features in the US constitution. Compare these features to the top features for three other countries of your choice. What do you notice?

<details>

<summary>Reveal code</summary>

```{r}

topfeatures(constitutions_dfm[docvars(constitutions_dfm)$country == "united_states_of_america",])

topfeatures(constitutions_dfm[docvars(constitutions_dfm)$country == "argentina",])

topfeatures(constitutions_dfm[docvars(constitutions_dfm)$country == "cuba",])

topfeatures(constitutions_dfm[docvars(constitutions_dfm)$country == "sudan",])

```

</details>

4.  Apply tf-idf weights to your dfm using the `dfm_tfidf()` function. Repeat the exercise above using the new matrix. What do you notice?

<details>

<summary>Reveal code</summary>

```{r}

constitutions_dfm_tf_idf <- constitutions_dfm %>% dfm_tfidf()

topfeatures(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == "united_states_of_america",])

topfeatures(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == "argentina",])

topfeatures(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == "cuba",])

topfeatures(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == "sudan",])

```

</details>

5.  Make two word clouds for two for the USA and one other country using the `textplot_wordcloud()` function. Marvel at how ugly these are.[^seminar3-6]

[^seminar3-6]: You may need to set the `min_count` argument to be a lower value than the default of 3 for the US constitution, as that text is very short.

<details>

<summary>Reveal code</summary>

```{r, warning=FALSE}

textplot_wordcloud(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == "united_states_of_america",], min_count = 0)

textplot_wordcloud(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == "cuba",], min_count = 2)

```

</details>

## Cosine Similarity

The cosine similarity ($cos(\theta)$) between two vectors $\textbf{a}$ and $\textbf{b}$ is defined as:

$$cos(\theta) = \frac{\mathbf{a} \cdot \mathbf{b}}{\left|\left| \mathbf{a} \right|\right| \left|\left| \mathbf{b} \right|\right|}$$

where $\theta$ is the angle between the two vectors and $\left| \mathbf{a} \right|$ and $\left| \mathbf{b} \right|$ are the *magnitudes* of the vectors $\mathbf{a}$ and $\mathbf{b}$, respectively. In slightly more laborious, but possibly easier to understand, notation:

$$cos(\theta) =  \frac{a_1b_1 + a_2b_2 + ... + a_Jb_J}{\sqrt{a_1^2 + a_2^2 + ... + a_J^2} \times \sqrt{b_1^2 + b_2^2 + ... + b_J^2}}$$

1.  Use the `textstat_simil()` function to calculate the cosine similarity between the preamble for the US constitution and *all* other preambles in the data.[^seminar3-9] Assign the output of this function to the original `constitutions` data.frame using the `as.numeric()` function. Which 3 constitutions are most similar to the US? Which are the 3 least similar?[^seminar3-10]

[^seminar3-9]: You can also provide this function with an `x` *matrix* and a `y` vector. This will enable you to calculate the similarity between all rows in `x` and the vector used for `y`.

[^seminar3-10]: Use the `order()` function to acheive this. Look back at seminar 2 if you have forgotten how to use this function.

<details>

<summary>Reveal code</summary>

```{r}

# Calculate the cosine similarity
cosine_sim <- textstat_simil(x = constitutions_dfm_tf_idf, 
                             y = constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == "united_states_of_america",],
                             method = "cosine")


# Assign variable to data.frame
constitutions$cosine_sim <- as.numeric(cosine_sim)

# Find the constitutions that are most and least similar
constitutions$country[order(constitutions$cosine_sim, decreasing = T)][1:3]
constitutions$country[order(constitutions$cosine_sim, decreasing = F)][1:3]

```

</details>

2.  Calculate the average cosine similarity between the constitution of the US and the constitutions of other countries for each decade in the data for all constitutions written from the 1950s onwards.

There are a couple of coding nuances that you will need to tackle to complete this question.

-   First, you will need to convert the `year` variable to a `decade` variable. You can do this by using the `%%` "modulo" operator, which calculates the remainder after the division of two numeric variables. For instance, `1986 %% 10` will return a value of `6`. If you subtract that from the original year, you will be left with the correct decade (i.e. `1986 - 6 = 1980`).

-   Second, you will need to calculate the decade-level averages of the cosine similarity variable that you created in answer to the question above. To do so, you should use the `group_by()` and `summarise()` functions.`group_by()` allows you to specify the variable by which the summarisation should be applied, and the `summarise()` function allows you to specify which type of summary you wish to use (i.e. here you should be using the `mean()` function).



<details>

<summary>Reveal code</summary>

```{r}

# Create the decade variable
constitutions$decade <- constitutions$year - (constitutions$year %% 10)

# Calculate the average similarity by decade
cosine_sim_by_year <- constitutions %>%
  filter(year >= 1950) %>%
  group_by(decade) %>%
  summarise(cosine_sim = mean(cosine_sim)) 

```

</details>

3.  Create a line graph (`geom_line()` in ggplot) with the averages that you calculated above on the y-axis and with the decades on the x-axis. Have constitution preambles become less similar to the preamble of the US constituion over recent history?

<details>

<summary>Reveal code</summary>

```{r}

cosine_sim_by_year %>%
  ggplot(aes(x = decade, y = cosine_sim)) + 
  geom_line() + 
  xlab("Decade") +
  ylab("Tf-idf cosine similarity with US constitution") + 
  theme_bw()


```

> There is no clear evidence that constitutions are less similar to the US's over time. This is contrary to the finding in [Law and Versteeg (2012)](https://www.nyulawreview.org/wp-content/uploads/2018/08/NYULawReview-87-3-Law-Versteeg_0.pdf), but consistent with other work in the area. For instance, [Elkins et. al., 2012](https://www.nyulawreview.org/online-features/comments-on-law-and-versteegs-the-declining-influence-of-the-united-states-constitution/) suggest that despite the addition of various "bells and whistles" in contemporary constitutions, "the influence of the U.S. Constitution remains evident."

</details>



## Exercise 10.2 - Naive Bayes Classification of Movie Revies

In this question, we will use Naive Bayes models to predict whether movies are positively or negatively reviewed. We will use a classic computer science dataset of movie reviews, [(Pang and Lee 2004)](http://www.cs.cornell.edu/home/llee/papers/cutsent.pdf). The movies corpus has an attribute `sentiment` that labels each text as either `pos` or `neg` according to the original imdb.com archived newspaper review star rating.  

You can extract the relevant corpus object using the following line of code:

```{r}

moviereviews <- quanteda.textmodels::data_corpus_moviereviews

```

Start by looking at the metadata included with this corpus using the `docvars()` function:

```{r, echpo = TRUE}

head(docvars(moviereviews))

```

We will be using the `sentiment` variable, which includes information from a human-labelling of movie reviews as either positive (`pos`) or negative (`neg`).

(a) Use the `table()` function to work out how many positive and how many negative movie reviews there are in the corpus.

<details>

<summary>Reveal code</summary>

```{r, echo = TRUE}

table(docvars(moviereviews)$sentiment)

```

</details>

b) Make a dfm for this corpus (i.e. `dfm()`), and make some reasonable feature selection decisions to reduce the number of features in the dfm. You will need to first convert the `moviereviews` corpus into a tokens object, using `tokens()`.

<details>

<summary>Reveal code</summary>

```{r}

movies_tokens <- tokens(moviereviews, 
                        remove_punct = TRUE, 
                        remove_numbers = TRUE, 
                        remove_symbols = TRUE)

movies_dfm <- dfm(movies_tokens) %>%
  dfm_remove(pattern = stopwords("en")) %>%
  dfm_trim(min_termfreq = 10)


```


</details>

(c) Use the code below to create a logical vector of the same length as the number of documents in the corpus. We will use this vector to define our training and test sets. Look at `?sample` to make sure you understand what each part of the code is doing. As we are using randomness to generate this vector, don't forget to first set your seed so that the results are fully replicable!

```{r}

set.seed(1234)

train <- sample(c(TRUE, FALSE), 2000, replace = TRUE, prob = c(.75, .25))

```

(d) Subset the dfm into a training set and a test set using the vector you just created. Use the `dfm_subset()` function to acheive this.

<details>

<summary>Reveal code</summary>

```{r}
movies_train_dfm <- dfm_subset(movies_dfm, train)
movies_test_dfm <- dfm_subset(movies_dfm, !train)

```

</details>


(e) Use the `textmodel_nb()` function to train the Naive Bayes classifier on the training dfm. You should use the dfm you created for the training corpus as the `x` argument to this function, and the outcome (i.e. `training_dfm$sentiment`) as the `y` argument.

<details>

<summary>Reveal code</summary>

```{r}

movie_nb <- textmodel_nb(x = movies_train_dfm, 
                         y = movies_train_dfm$sentiment)

```

</details>

(f) Examine the `param` element of the fitted model. Which words have the highest probability under the `pos` class? Which words have the highest probability under the `neg` class? You might find the `sort()` function helpful here.

<details>

<summary>Reveal code</summary>

```{r}

head(sort(movie_nb$param[2,], decreasing = TRUE), 40)

head(sort(movie_nb$param[1,], decreasing = TRUE), 40)

```

</details>

(g) Use the `predict()` function to predict the sentiment of movies in the test set dfm. The predict function takes two arguments in this instance: 1) the estimated Naive Bayes model from part (e), and 2) the test-set dfm. Create a confusion matrix of the predicted classes and the actual classes in the test data. What is the accuracy of your model?

<details>

<summary>Reveal code</summary>

```{r}

movie_test_predicted_class <- predict(movie_nb, 
                                      newdata = movies_test_dfm)

movie_confusion <- table(movie_test_predicted_class, movies_test_dfm$sentiment)

movie_confusion

## Accuracy
mean(movie_test_predicted_class == movies_test_dfm$sentiment)

```

</details>

(h) Use the `confusionMatrix()` function to calculate other statistics relevant to the predictive performance of your model. The first argument to the `confusionMatrix()` function should be the confusion matrix that you created in answer to question (g). You should also set the `positive` argument equal to `"pos"` to tell R the level of the outcome that corresponds to a "positive" result. Report the the accuracy, sensitivity and specificity of your predictions, giving a brief interpretation of each.

<details>

<summary>Reveal code</summary>

```{r}

movie_confusion_statistics <- confusionMatrix(movie_confusion, positive = "pos")

movie_confusion_statistics

```

**Accuracy: The proportion of observations correctly classified is `r movie_confusion_statistics$overall[1]`.**

**Sensitivity: The proportion of "positive" movie reviews correctly classified is `r movie_confusion_statistics$byClass[1]`.**

**Specificity: The proportion of "negative" movie reviews correctly classified is `r movie_confusion_statistics$byClass[2]`.**

</details>

